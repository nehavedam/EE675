{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562a8b4-3dec-4514-9653-74f7f580d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee17b52-a0da-4cd0-86c2-15776b779bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e09fa3-6794-40f4-abc1-3aa71bb0bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural data shape: (496, 170, 67)\n",
      "Kinematic data shape: (496, 170, 6)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('prepared_data.mat')\n",
    "neural_data_tensor = data['neural_data_tensor']  \n",
    "kinematic_tensor = data['kinematic_tensor']      \n",
    "mean_spikes = data['mean_spikes'].flatten()      \n",
    "std_spikes = data['std_spikes'].flatten()        \n",
    "print(f\"Neural data shape: {neural_data_tensor.shape}\")\n",
    "print(f\"Kinematic data shape: {kinematic_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7260e01f-7d5f-42c4-8966-8fff222246bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (297, 170, 67), Validation shape: (99, 170, 67), Test shape: (100, 170, 67)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "reaches = neural_data_tensor.shape[0]\n",
    "train_idx, test_idx = train_test_split(np.arange(reaches), test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.25, random_state=42)  # 20% validation\n",
    "X_train = neural_data_tensor[train_idx]\n",
    "Y_train = kinematic_tensor[train_idx]\n",
    "X_val = neural_data_tensor[val_idx]\n",
    "Y_val = kinematic_tensor[val_idx]\n",
    "X_test = neural_data_tensor[test_idx]\n",
    "Y_test = kinematic_tensor[test_idx]\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eeb4e43-2c2f-473e-afb0-ce9ec7620404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralDecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(NeuralDecoderRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[2]  \n",
    "hidden_size = 64               \n",
    "output_size = Y_train.shape[2] \n",
    "num_layers = 1                 \n",
    "\n",
    "model = NeuralDecoderRNN(input_size, hidden_size, output_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235deb7c-23ab-4a55-9d3e-e98947478163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_torch = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_torch = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_torch, Y_train_torch), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_torch, Y_val_torch), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2870df82-ca9b-4a59-a512-94b49c7350b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 549.7490, Validation Loss: 588.3491\n",
      "Epoch 2/200, Train Loss: 543.8764, Validation Loss: 587.4729\n",
      "Epoch 3/200, Train Loss: 550.7702, Validation Loss: 585.7706\n",
      "Epoch 4/200, Train Loss: 541.1102, Validation Loss: 581.7910\n",
      "Epoch 5/200, Train Loss: 538.7673, Validation Loss: 576.7584\n",
      "Epoch 6/200, Train Loss: 545.6488, Validation Loss: 568.8636\n",
      "Epoch 7/200, Train Loss: 527.6658, Validation Loss: 563.1033\n",
      "Epoch 8/200, Train Loss: 526.1552, Validation Loss: 554.2939\n",
      "Epoch 9/200, Train Loss: 515.1100, Validation Loss: 548.4693\n",
      "Epoch 10/200, Train Loss: 507.2189, Validation Loss: 541.0808\n",
      "Epoch 11/200, Train Loss: 500.6980, Validation Loss: 535.0841\n",
      "Epoch 12/200, Train Loss: 489.7675, Validation Loss: 528.8688\n",
      "Epoch 13/200, Train Loss: 496.4371, Validation Loss: 522.9589\n",
      "Epoch 14/200, Train Loss: 480.4661, Validation Loss: 516.8959\n",
      "Epoch 15/200, Train Loss: 475.7865, Validation Loss: 513.7797\n",
      "Epoch 16/200, Train Loss: 470.4595, Validation Loss: 507.4437\n",
      "Epoch 17/200, Train Loss: 464.2087, Validation Loss: 503.4366\n",
      "Epoch 18/200, Train Loss: 463.0759, Validation Loss: 498.8015\n",
      "Epoch 19/200, Train Loss: 456.5884, Validation Loss: 494.5960\n",
      "Epoch 20/200, Train Loss: 453.2329, Validation Loss: 491.7370\n",
      "Epoch 21/200, Train Loss: 449.9954, Validation Loss: 488.3053\n",
      "Epoch 22/200, Train Loss: 452.6688, Validation Loss: 483.7090\n",
      "Epoch 23/200, Train Loss: 437.7202, Validation Loss: 482.4235\n",
      "Epoch 24/200, Train Loss: 438.5385, Validation Loss: 477.2070\n",
      "Epoch 25/200, Train Loss: 433.0960, Validation Loss: 472.8844\n",
      "Epoch 26/200, Train Loss: 428.3281, Validation Loss: 473.5935\n",
      "Epoch 27/200, Train Loss: 418.0150, Validation Loss: 470.6666\n",
      "Epoch 28/200, Train Loss: 419.5105, Validation Loss: 465.8999\n",
      "Epoch 29/200, Train Loss: 425.9970, Validation Loss: 462.5198\n",
      "Epoch 30/200, Train Loss: 418.4335, Validation Loss: 461.2888\n",
      "Epoch 31/200, Train Loss: 412.7704, Validation Loss: 458.1394\n",
      "Epoch 32/200, Train Loss: 406.5922, Validation Loss: 455.9570\n",
      "Epoch 33/200, Train Loss: 399.1301, Validation Loss: 454.8838\n",
      "Epoch 34/200, Train Loss: 400.9520, Validation Loss: 450.6502\n",
      "Epoch 35/200, Train Loss: 388.2658, Validation Loss: 449.5605\n",
      "Epoch 36/200, Train Loss: 395.6827, Validation Loss: 449.0267\n",
      "Epoch 37/200, Train Loss: 389.0656, Validation Loss: 445.2959\n",
      "Epoch 38/200, Train Loss: 384.8329, Validation Loss: 444.5125\n",
      "Epoch 39/200, Train Loss: 381.2305, Validation Loss: 443.1886\n",
      "Epoch 40/200, Train Loss: 386.1112, Validation Loss: 442.7111\n",
      "Epoch 41/200, Train Loss: 376.2252, Validation Loss: 437.8384\n",
      "Epoch 42/200, Train Loss: 384.5068, Validation Loss: 440.5774\n",
      "Epoch 43/200, Train Loss: 375.9218, Validation Loss: 440.8429\n",
      "Epoch 44/200, Train Loss: 377.9998, Validation Loss: 436.9010\n",
      "Epoch 45/200, Train Loss: 373.8103, Validation Loss: 432.9131\n",
      "Epoch 46/200, Train Loss: 370.9681, Validation Loss: 430.7195\n",
      "Epoch 47/200, Train Loss: 375.0788, Validation Loss: 431.6981\n",
      "Epoch 48/200, Train Loss: 363.6479, Validation Loss: 429.8067\n",
      "Epoch 49/200, Train Loss: 359.6240, Validation Loss: 430.9940\n",
      "Epoch 50/200, Train Loss: 359.5697, Validation Loss: 427.1901\n",
      "Epoch 51/200, Train Loss: 359.8754, Validation Loss: 425.5084\n",
      "Epoch 52/200, Train Loss: 351.5674, Validation Loss: 425.7483\n",
      "Epoch 53/200, Train Loss: 351.6586, Validation Loss: 423.3233\n",
      "Epoch 54/200, Train Loss: 351.8134, Validation Loss: 423.1109\n",
      "Epoch 55/200, Train Loss: 354.9804, Validation Loss: 420.9833\n",
      "Epoch 56/200, Train Loss: 343.8075, Validation Loss: 424.4518\n",
      "Epoch 57/200, Train Loss: 343.8403, Validation Loss: 424.2393\n",
      "Epoch 58/200, Train Loss: 341.5144, Validation Loss: 421.2641\n",
      "Epoch 59/200, Train Loss: 341.2934, Validation Loss: 419.3183\n",
      "Epoch 60/200, Train Loss: 341.2759, Validation Loss: 419.8058\n",
      "Epoch 61/200, Train Loss: 334.4754, Validation Loss: 420.9859\n",
      "Epoch 62/200, Train Loss: 330.0283, Validation Loss: 421.7511\n",
      "Epoch 63/200, Train Loss: 341.1296, Validation Loss: 421.3953\n",
      "Epoch 64/200, Train Loss: 334.3712, Validation Loss: 415.0552\n",
      "Epoch 65/200, Train Loss: 331.1663, Validation Loss: 420.7719\n",
      "Epoch 66/200, Train Loss: 334.2009, Validation Loss: 412.7516\n",
      "Epoch 67/200, Train Loss: 335.2263, Validation Loss: 413.7961\n",
      "Epoch 68/200, Train Loss: 331.0421, Validation Loss: 416.7873\n",
      "Epoch 69/200, Train Loss: 326.6779, Validation Loss: 409.6584\n",
      "Epoch 70/200, Train Loss: 321.4298, Validation Loss: 407.8718\n",
      "Epoch 71/200, Train Loss: 320.9888, Validation Loss: 410.9753\n",
      "Epoch 72/200, Train Loss: 322.2698, Validation Loss: 406.6449\n",
      "Epoch 73/200, Train Loss: 316.7840, Validation Loss: 408.1717\n",
      "Epoch 74/200, Train Loss: 317.1220, Validation Loss: 403.8386\n",
      "Epoch 75/200, Train Loss: 318.2973, Validation Loss: 407.5412\n",
      "Epoch 76/200, Train Loss: 315.8579, Validation Loss: 411.3121\n",
      "Epoch 77/200, Train Loss: 311.4210, Validation Loss: 411.8175\n",
      "Epoch 78/200, Train Loss: 314.5257, Validation Loss: 408.4911\n",
      "Epoch 79/200, Train Loss: 310.6358, Validation Loss: 409.6535\n",
      "Epoch 80/200, Train Loss: 312.3911, Validation Loss: 409.4621\n",
      "Epoch 81/200, Train Loss: 304.2632, Validation Loss: 407.2291\n",
      "Epoch 82/200, Train Loss: 305.0590, Validation Loss: 410.0748\n",
      "Epoch 83/200, Train Loss: 305.9226, Validation Loss: 410.0397\n",
      "Epoch 84/200, Train Loss: 301.8114, Validation Loss: 408.6649\n",
      "Epoch 85/200, Train Loss: 307.3944, Validation Loss: 410.3476\n",
      "Epoch 86/200, Train Loss: 297.5944, Validation Loss: 408.7163\n",
      "Epoch 87/200, Train Loss: 299.4477, Validation Loss: 405.3922\n",
      "Epoch 88/200, Train Loss: 304.5761, Validation Loss: 408.1314\n",
      "Epoch 89/200, Train Loss: 298.8440, Validation Loss: 407.3913\n",
      "Epoch 90/200, Train Loss: 293.3449, Validation Loss: 404.5780\n",
      "Epoch 91/200, Train Loss: 297.2134, Validation Loss: 408.3466\n",
      "Epoch 92/200, Train Loss: 299.3864, Validation Loss: 410.6750\n",
      "Epoch 93/200, Train Loss: 297.3358, Validation Loss: 404.9112\n",
      "Epoch 94/200, Train Loss: 295.3692, Validation Loss: 408.8649\n",
      "Epoch 95/200, Train Loss: 292.1977, Validation Loss: 406.4871\n",
      "Epoch 96/200, Train Loss: 295.7619, Validation Loss: 408.6029\n",
      "Epoch 97/200, Train Loss: 290.1188, Validation Loss: 409.3178\n",
      "Epoch 98/200, Train Loss: 296.0864, Validation Loss: 408.6764\n",
      "Epoch 99/200, Train Loss: 287.4116, Validation Loss: 405.1252\n",
      "Epoch 100/200, Train Loss: 285.7920, Validation Loss: 408.3997\n",
      "Epoch 101/200, Train Loss: 288.1895, Validation Loss: 403.0173\n",
      "Epoch 102/200, Train Loss: 287.5129, Validation Loss: 404.0758\n",
      "Epoch 103/200, Train Loss: 285.9645, Validation Loss: 401.7908\n",
      "Epoch 104/200, Train Loss: 287.7213, Validation Loss: 405.4961\n",
      "Epoch 105/200, Train Loss: 284.2765, Validation Loss: 398.2501\n",
      "Epoch 106/200, Train Loss: 283.3181, Validation Loss: 405.0578\n",
      "Epoch 107/200, Train Loss: 279.3122, Validation Loss: 407.3184\n",
      "Epoch 108/200, Train Loss: 277.5924, Validation Loss: 408.1882\n",
      "Epoch 109/200, Train Loss: 275.7754, Validation Loss: 406.8916\n",
      "Epoch 110/200, Train Loss: 277.4518, Validation Loss: 401.5026\n",
      "Epoch 111/200, Train Loss: 275.1676, Validation Loss: 407.8690\n",
      "Epoch 112/200, Train Loss: 275.5995, Validation Loss: 405.0969\n",
      "Epoch 113/200, Train Loss: 274.3484, Validation Loss: 399.1533\n",
      "Epoch 114/200, Train Loss: 278.9829, Validation Loss: 410.5823\n",
      "Epoch 115/200, Train Loss: 273.1701, Validation Loss: 401.5232\n",
      "Epoch 116/200, Train Loss: 272.8790, Validation Loss: 400.4717\n",
      "Epoch 117/200, Train Loss: 275.2107, Validation Loss: 403.8272\n",
      "Epoch 118/200, Train Loss: 272.8772, Validation Loss: 403.6428\n",
      "Epoch 119/200, Train Loss: 271.4851, Validation Loss: 410.1060\n",
      "Epoch 120/200, Train Loss: 268.7268, Validation Loss: 407.4009\n",
      "Epoch 121/200, Train Loss: 264.0499, Validation Loss: 407.2635\n",
      "Epoch 122/200, Train Loss: 263.8537, Validation Loss: 407.7038\n",
      "Epoch 123/200, Train Loss: 267.4766, Validation Loss: 405.7616\n",
      "Epoch 124/200, Train Loss: 268.1449, Validation Loss: 400.4553\n",
      "Epoch 125/200, Train Loss: 268.3879, Validation Loss: 405.6103\n",
      "Epoch 126/200, Train Loss: 261.5097, Validation Loss: 404.4196\n",
      "Epoch 127/200, Train Loss: 262.8694, Validation Loss: 402.3150\n",
      "Epoch 128/200, Train Loss: 265.9814, Validation Loss: 402.9862\n",
      "Epoch 129/200, Train Loss: 264.4596, Validation Loss: 393.6025\n",
      "Epoch 130/200, Train Loss: 259.0392, Validation Loss: 405.2542\n",
      "Epoch 131/200, Train Loss: 260.5717, Validation Loss: 398.5881\n",
      "Epoch 132/200, Train Loss: 261.4510, Validation Loss: 407.4578\n",
      "Epoch 133/200, Train Loss: 258.5185, Validation Loss: 410.3776\n",
      "Epoch 134/200, Train Loss: 259.6953, Validation Loss: 405.7060\n",
      "Epoch 135/200, Train Loss: 267.5801, Validation Loss: 407.1631\n",
      "Epoch 136/200, Train Loss: 257.6799, Validation Loss: 408.0856\n",
      "Epoch 137/200, Train Loss: 258.1401, Validation Loss: 411.3821\n",
      "Epoch 138/200, Train Loss: 255.5726, Validation Loss: 406.8674\n",
      "Epoch 139/200, Train Loss: 257.5839, Validation Loss: 403.5575\n",
      "Epoch 140/200, Train Loss: 250.7441, Validation Loss: 411.0595\n",
      "Epoch 141/200, Train Loss: 255.2338, Validation Loss: 412.9335\n",
      "Epoch 142/200, Train Loss: 254.5261, Validation Loss: 410.7977\n",
      "Epoch 143/200, Train Loss: 253.5809, Validation Loss: 402.0345\n",
      "Epoch 144/200, Train Loss: 254.9235, Validation Loss: 408.1610\n",
      "Epoch 145/200, Train Loss: 251.6286, Validation Loss: 410.5315\n",
      "Epoch 146/200, Train Loss: 255.7649, Validation Loss: 407.5344\n",
      "Epoch 147/200, Train Loss: 255.7646, Validation Loss: 407.1758\n",
      "Epoch 148/200, Train Loss: 249.9471, Validation Loss: 400.5617\n",
      "Epoch 149/200, Train Loss: 255.4531, Validation Loss: 408.4219\n",
      "Epoch 150/200, Train Loss: 245.7771, Validation Loss: 403.2733\n",
      "Epoch 151/200, Train Loss: 248.6035, Validation Loss: 400.7282\n",
      "Epoch 152/200, Train Loss: 249.3449, Validation Loss: 402.6891\n",
      "Epoch 153/200, Train Loss: 247.4994, Validation Loss: 404.9470\n",
      "Epoch 154/200, Train Loss: 247.0364, Validation Loss: 406.6267\n",
      "Epoch 155/200, Train Loss: 244.4566, Validation Loss: 406.6693\n",
      "Epoch 156/200, Train Loss: 243.9550, Validation Loss: 401.0135\n",
      "Epoch 157/200, Train Loss: 245.3048, Validation Loss: 406.8922\n",
      "Epoch 158/200, Train Loss: 245.5777, Validation Loss: 403.9317\n",
      "Epoch 159/200, Train Loss: 244.5088, Validation Loss: 403.5780\n",
      "Epoch 160/200, Train Loss: 241.6665, Validation Loss: 405.1091\n",
      "Epoch 161/200, Train Loss: 241.6025, Validation Loss: 404.1846\n",
      "Epoch 162/200, Train Loss: 240.0417, Validation Loss: 405.2832\n",
      "Epoch 163/200, Train Loss: 242.2238, Validation Loss: 409.7679\n",
      "Epoch 164/200, Train Loss: 240.7022, Validation Loss: 402.8622\n",
      "Epoch 165/200, Train Loss: 243.6853, Validation Loss: 407.0655\n",
      "Epoch 166/200, Train Loss: 239.6258, Validation Loss: 408.7071\n",
      "Epoch 167/200, Train Loss: 240.9817, Validation Loss: 408.4473\n",
      "Epoch 168/200, Train Loss: 241.0660, Validation Loss: 410.8461\n",
      "Epoch 169/200, Train Loss: 238.9386, Validation Loss: 399.1048\n",
      "Epoch 170/200, Train Loss: 240.2588, Validation Loss: 405.9117\n",
      "Epoch 171/200, Train Loss: 240.7194, Validation Loss: 407.7915\n",
      "Epoch 172/200, Train Loss: 237.1030, Validation Loss: 406.0915\n",
      "Epoch 173/200, Train Loss: 234.8569, Validation Loss: 405.4257\n",
      "Epoch 174/200, Train Loss: 233.1148, Validation Loss: 405.7248\n",
      "Epoch 175/200, Train Loss: 235.3458, Validation Loss: 408.5167\n",
      "Epoch 176/200, Train Loss: 231.1793, Validation Loss: 405.5359\n",
      "Epoch 177/200, Train Loss: 234.0285, Validation Loss: 412.5305\n",
      "Epoch 178/200, Train Loss: 233.1092, Validation Loss: 404.6898\n",
      "Epoch 179/200, Train Loss: 231.8546, Validation Loss: 412.8982\n",
      "Epoch 180/200, Train Loss: 234.9078, Validation Loss: 410.5431\n",
      "Epoch 181/200, Train Loss: 230.5458, Validation Loss: 409.7565\n",
      "Epoch 182/200, Train Loss: 232.6961, Validation Loss: 411.0927\n",
      "Epoch 183/200, Train Loss: 231.9711, Validation Loss: 417.0760\n",
      "Epoch 184/200, Train Loss: 226.8980, Validation Loss: 417.2486\n",
      "Epoch 185/200, Train Loss: 232.1454, Validation Loss: 409.0189\n",
      "Epoch 186/200, Train Loss: 232.0415, Validation Loss: 409.9713\n",
      "Epoch 187/200, Train Loss: 226.0100, Validation Loss: 405.8492\n",
      "Epoch 188/200, Train Loss: 229.1069, Validation Loss: 406.1633\n",
      "Epoch 189/200, Train Loss: 227.3489, Validation Loss: 410.8025\n",
      "Epoch 190/200, Train Loss: 225.8271, Validation Loss: 406.6550\n",
      "Epoch 191/200, Train Loss: 228.5220, Validation Loss: 414.7403\n",
      "Epoch 192/200, Train Loss: 227.2681, Validation Loss: 411.4085\n",
      "Epoch 193/200, Train Loss: 230.9762, Validation Loss: 413.7358\n",
      "Epoch 194/200, Train Loss: 235.1470, Validation Loss: 421.3136\n",
      "Epoch 195/200, Train Loss: 254.1969, Validation Loss: 407.1929\n",
      "Epoch 196/200, Train Loss: 254.9747, Validation Loss: 392.1053\n",
      "Epoch 197/200, Train Loss: 254.1582, Validation Loss: 392.2442\n",
      "Epoch 198/200, Train Loss: 248.2528, Validation Loss: 395.7692\n",
      "Epoch 199/200, Train Loss: 244.2654, Validation Loss: 387.8430\n",
      "Epoch 200/200, Train Loss: 237.7404, Validation Loss: 394.0850\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e35e608-c372-41b1-a345-8692bcbccc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 381.4996\n",
      "Validation MAE: 296.3833, R²: 0.3070\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_torch = torch.tensor(Y_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_torch)\n",
    "    test_loss = criterion(predictions, Y_test_torch)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "preds = predictions.detach().cpu().numpy() * kin_std + kin_mean  \n",
    "targets = Y_test_torch.detach().cpu().numpy() * kin_std + kin_mean  \n",
    "mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
    "r2 = r2_score(targets.flatten(), preds.flatten())\n",
    "\n",
    "print(f\"Validation MAE: {mae:.4f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496f198-42a1-4ee7-80ab-b2a3837ddc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"neural_decoder_rnn.pth\")\n",
    "\n",
    "# Load model\n",
    "model = NeuralDecoderRNN(input_size, hidden_size, output_size, num_layers)\n",
    "model.load_state_dict(torch.load(\"neural_decoder_rnn.pth\"))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
